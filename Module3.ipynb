{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ab4474-b00b-4866-ba65-568845a68f6e",
   "metadata": {},
   "source": [
    "### UBC Extended Learning\n",
    "#### Instructor: Socorro Dominguez\n",
    "#### Module 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df87f1-95d9-443f-90ad-f6e2dcf60196",
   "metadata": {},
   "source": [
    "- [] Split a dataset into train and test sets using train_test_split function.\n",
    "- [] Explain the difference between train, validation, test, and \"deployment\" data.\n",
    "- [] Identify the difference between training error, validation error, and test error.\n",
    "- [] Explain cross-validation and use cross_val_score() and cross_validate() to calculate cross-validation error.\n",
    "- [] Explain overfitting, underfitting, and the fundamental tradeoff.\n",
    "- [] State the golden rule and identify the scenarios when it's violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad493f7-f4c9-456a-854c-e9b531b99d84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Test/train split\n",
    "\n",
    "\n",
    "- If you were studying from practice exams, what would you do to get an idea of how you will perform on the real one?\n",
    "- Idea:\n",
    "    - Goal is to *learn the underlying concepts* rather than *memorize the questions/answers*\n",
    "        1. Learn from some practice exams / questions that you do again and again (training data)\n",
    "        2. Write a *new* practice exam that you left to the side (test data)\n",
    "- This strategy will give you an idea of how well you will generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d8854-7619-4018-b36d-1f7876b47dab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "- With more powerful models, we can always arbitrarily fit the training data better and better (lower training error)\n",
    "- What we're actually trying to achieve is good performance on **new data that our model was never trained on**\n",
    "- We can train our model on just a subset of our whole dataset (**train set**) and evaluate its performance on the rest (**test set**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fb5a8-a192-4598-bdbe-dfd932286d49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/test_set.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "![](images/overfitting_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf40b5-e96e-4636-aa3e-8c336f172fad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Overfitting & Underfitting\n",
    "\n",
    "- Imagine you only had to study one exam - and then, the real exam are the exact same questions as the mock test (and you know this)\n",
    "\n",
    "- Later on, if you had a different exam, would you feel just as confient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543499b7-8dbf-4838-ad14-e8c2e78736bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/overfitting.png)\n",
    "- Data has two components: signal (pattern) + noise\n",
    "- Example: predicting house prices from # of bedrooms, area, age, etc.\n",
    "    - Signal: degree to which these features influence the price\n",
    "    - Noise: random variation, or variation due to unknown features\n",
    "- When the model is fitting the noise, it is overfitting\n",
    "- **Overfitting** is when the model produces an analysis that corresponds too closely or exactly to a particular set of data, and may fail to fit to additional data or predict future observations reliably. The model is memorizing noise.\n",
    "- **Underfitting** is when a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba08890-ab48-4650-ba1a-2c9e65f5c197",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling bias\n",
    "\n",
    "- Remember that the model just learns patterns in the data you give it.\n",
    "- If our dataset isn't representative of the type of data we want our model to handle afterwards, it is not going to perform well.\n",
    "- Most machine learning models fail dramatically for **out of distribution** data that is very different from what we've trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383e3ac-a21a-40b9-8cf3-dfc0d162987c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example\n",
    "You studied very hard for a test. And you were very comfortable with the mock exams. \n",
    "But the day of the test, you are very nervous and you don't perform very well.\n",
    "\n",
    "You never studied, and you partied the night before the test. You barely showed up to the test but the test was extremely easy and the few questions you didn't know, they were multiple choice and luck was on your side. You achieve a perfect score.\n",
    "\n",
    "If you only take that **one** test, is it enough to really measure your knowledge?\n",
    "\n",
    "Essentially **all** datasets are biased one way or another. Train yourself to think of different ways\n",
    "that they could be biased and different reasons why. Understanding the conditions under which your model\n",
    "will work and fail is essential when deploying it in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605a69b-5f7f-4ea1-a880-9b16d80b54f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cross-validation\n",
    "- We wish to evaluate our model on a dataset with little sampling bias, so that we know how it will perform in the real world\n",
    "- Smaller datasets are far more likely to suffer from sampling bias\n",
    "- This creates a conflict:\n",
    "    - We want to have a big test set to get a realistic estimate of our model's performance without sampling bias\n",
    "    - We want to have a big training set so that our model can learn without sampling bias\n",
    "- How can we reconcile these goals?\n",
    "- **One answer:** cross-validation. Even if any single fold suffers from sampling bias, at least we are averaging the results from many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973c3a9-9993-471f-9df7-19a13a51ee49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/cross_validation.png\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d238a6-b336-476b-923b-1ee48bd01dfd",
   "metadata": {},
   "source": [
    "#### The Golden Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3c6e3-7c3d-4fb6-b128-abca90f2bc4d",
   "metadata": {},
   "source": [
    "Even though we care the most about test score:\n",
    "\n",
    "**THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126aa6c2-f9ed-4094-a52c-01c9113b6c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train / validation / test split\n",
    "\n",
    "**Question**: Have you noticed anything wrong with what we are doing? Should we trust our cross-validation score as a true estimate of performance on out-of-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d8d51-8c54-4d34-852d-df85a5e126db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have been using the cross-validation score to select hyperparameters. You can think of this as having used the validation data in each fold to train our model in a way. So, our model will be biased to the dataset and our cross-validation score will be overly-optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8c321-5dc2-478a-b5d1-e6c1ab829603",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution**:\n",
    "1. Split the dataset into a train/test set\n",
    "2. Perform grid search with cross-validation on the training set. Each fold, the training set will be split into a train/validation set\n",
    "3. Using the hyperparameters that obtained the best cross-validation score, retrain the model on the entire training set (not just training folds within this training set)\n",
    "4. Evaluate the model on the test set to get a true estimate of the out-of-sample error\n",
    "\n",
    "<img src=\"images/cross_validation_with_test.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5c1f5-5e90-4505-8f11-bb21d6723000",
   "metadata": {},
   "source": [
    "\n",
    "To not break the golden rule:\n",
    "- Do any EDA after splitting\n",
    "- Do any data preprocessing after splitting.\n",
    "- Module 5 we will talk about pipelines - this is going to be helpful too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992879fa-a1f3-43ac-ae56-b00a7df9f1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
