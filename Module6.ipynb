{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcd098b-c712-4b36-aefb-a40e339c6d32",
   "metadata": {},
   "source": [
    "### UBC Extended Learning\n",
    "#### Instructor: Socorro Dominguez\n",
    "#### Module 06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000f2c1-982d-426e-986f-d393d89b12fe",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "- [] Explain handle_unknown=\"ignore\" hyperparameter of scikit-learn's OneHotEncoder.\n",
    "- [] Identify when it's appropriate to apply ordinal encoding vs one-hot encoding.\n",
    "- [] Explain strategies to deal with categorical variables with too many categories.\n",
    "- [] Explain why text data needs a different treatment than categorical variables.\n",
    "- [] Use scikit-learn's CountVectorizer to encode text data.\n",
    "- [] Explain different hyperparameters of CountVectorizer.\n",
    "- [] Use ColumnTransformer to build all our transformations together into one object and use it with scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a842f-b4c9-411a-aa05-7d94e879918b",
   "metadata": {},
   "source": [
    "#### One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e4feaf-dbf4-4fc8-960f-47186e123c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded training data:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Encoded test data:\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example data with known categories during training\n",
    "X_train = [['cat'], ['dog'], ['fish']]\n",
    "\n",
    "# Example data with a new category during testing\n",
    "X_test = [['cat'], ['bird'], ['fish']]\n",
    "\n",
    "# Create the OneHotEncoder with handle_unknown='ignore'\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "print(\"Encoded training data:\")\n",
    "print(X_train_encoded.toarray())\n",
    "\n",
    "print(\"Encoded test data:\")\n",
    "print(X_test_encoded.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024081b9-4ff2-4602-981d-d48cf69de3b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding Bag of Words (CountVectorizer) in Python\n",
    "\n",
    "- Bag of Words is a fundamental concept in Natural Language Processing (NLP) used for text analysis and feature extraction.\n",
    "- Bag of Words (BoW) is a technique that converts text documents into numerical feature vectors.\n",
    "- It represents a document as a collection (or \"bag\") of words, disregarding grammar and word order but keeping track of word frequency.\n",
    "- BoW is widely used for text classification, sentiment analysis, and clustering tasks in NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f33b4c-afdf-4bb6-9a89-fb2a5a45b6c1",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1200/1*3K9GIOVLNu0cRvQap_KaRg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89eb31-467a-4770-8a78-1f99af4fbc16",
   "metadata": {},
   "source": [
    "- Bag of Words works in the following steps:\n",
    "  1. Tokenization: Break down the text into individual words or tokens.\n",
    "  2. Counting: Count the occurrence of each word in the document.\n",
    "  3. Vectorization: Convert the word counts into numerical feature vectors.\n",
    "\n",
    "#### Example Text Document\n",
    "\n",
    "- Let's consider an example text document: \"The cat sat on the mat. The mat was blue.\"\n",
    "- We will apply Bag of Words to this document to convert it into a numerical representation.\n",
    "\n",
    "- The tokenization process separates the document into individual words: \n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"The\", \"mat\", \"was\", \"blue\"].\n",
    "\n",
    "- Counting the occurrences of each word, we get: {\"The\": 2, \"cat\": 1, \"sat\": 1, \"on\": 1, \"the\": 1, \"mat\": 2, \"was\": 1, \"blue\": 1}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22574340-1dbb-4365-9cec-c67d98ff8514",
   "metadata": {},
   "source": [
    "#### CountVectorizer in Python \n",
    "\n",
    "- CountVectorizer in Python is a powerful tool that automates the process of Bag of Words.\n",
    "- It converts a collection of text documents to a matrix of token counts.\n",
    "- Let's see how to implement Bag of Words using CountVectorizer in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793c3e68-eecb-43b2-ad58-1cf1f5d0795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names (tokens): ['and' 'blue' 'cat' 'happy' 'mat' 'on' 'sat' 'the' 'was']\n",
      "Numerical representation of documents:\n",
      "[[0 0 1 0 1 1 1 2 0]\n",
      " [1 1 1 1 1 0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example text documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The mat was blue and the cat was happy.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names and the numerical representation of the documents\n",
    "print(\"Feature names (tokens):\", feature_names)\n",
    "print(\"Numerical representation of documents:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd604f29-0aea-4bd5-b552-ad45babceaa0",
   "metadata": {},
   "source": [
    "#### Applications of Bag of Words\n",
    "- Bag of Words has numerous applications in NLP, including:\n",
    "  - Text classification: Identifying the topic or sentiment of a document.\n",
    "  - Document clustering: Grouping similar documents together.\n",
    "  - Information retrieval: Ranking documents based on relevance to a query.\n",
    "\n",
    "\n",
    "#### Two important hyperparameters:\n",
    "- stop_words : Does not take into consideration words that are not meaningful (is, the, a, ...)\n",
    "- max_features : Some vocabularies can be massive, this can make things SLOW. Use this to make the process faster, at least when you set up the pipeline.\n",
    "\n",
    "#### Remember\n",
    "- Bag of Words is a powerful technique for converting text data into numerical representations.\n",
    "- CountVectorizer in Python simplifies the implementation of Bag of Words.\n",
    "- Understanding Bag of Words is essential for various NLP tasks, enabling efficient text analysis and feature extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
