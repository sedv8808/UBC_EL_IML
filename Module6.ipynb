{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcd098b-c712-4b36-aefb-a40e339c6d32",
   "metadata": {},
   "source": [
    "### UBC Extended Learning\n",
    "#### Instructor: Socorro Dominguez\n",
    "#### Module 06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000f2c1-982d-426e-986f-d393d89b12fe",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "- [] Explain handle_unknown=\"ignore\" hyperparameter of scikit-learn's OneHotEncoder.\n",
    "- [] Identify when it's appropriate to apply ordinal encoding vs one-hot encoding.\n",
    "- [] Explain strategies to deal with categorical variables with too many categories.\n",
    "- [] Explain why text data needs a different treatment than categorical variables.\n",
    "- [] Use scikit-learn's CountVectorizer to encode text data.\n",
    "- [] Explain different hyperparameters of CountVectorizer.\n",
    "- [] Use ColumnTransformer to build all our transformations together into one object and use it with scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2dbf2-dc2d-41de-aa57-63fd94cc29de",
   "metadata": {},
   "source": [
    "#### One Hot Encoder\n",
    "- Converts categorical variables into a binary matrix (one-hot encoded).\n",
    "- Each category becomes a binary column, indicating its presence (1) or absence (0).\n",
    "\n",
    "**Example:**\n",
    "   - Original Data: ['Red', 'Blue', 'Green']\n",
    "   - One-Hot Encoded:\n",
    "     ```markdown\n",
    "     Red   Blue   Green\n",
    "     1     0      0\n",
    "     0     1      0\n",
    "     0     0      1\n",
    "     ```\n",
    "\n",
    "- Avoids introducing false ordinal relationships in nominal categorical data.\n",
    "\n",
    "\n",
    "- The `handle_unknown` determines how the encoder handles unknown categories during transformation.\n",
    "\n",
    "- When set to \"ignore,\" the encoder will not raise an error if it encounters an unknown category during transformation.\n",
    "\n",
    "- Instead, it will ignore the unknown category and assign zeros to all corresponding one-hot encoded columns.\n",
    "\n",
    "- If the above example, suddenly found a `Yellow` observation:\n",
    "\n",
    "     ```markdown\n",
    "     Red   Blue   Green\n",
    "     1     0      0\n",
    "     0     1      0\n",
    "     0     0      1\n",
    "     0     0      0\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e4feaf-dbf4-4fc8-960f-47186e123c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded training data:\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "Encoded test data:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example data with known categories during training\n",
    "X_train = [['R'], ['G'], ['B']]\n",
    "\n",
    "# Example data with a new category during testing\n",
    "X_test = [['R'], ['Y'], ['Y']]\n",
    "\n",
    "# Create the OneHotEncoder with handle_unknown='ignore'\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "print(\"Encoded training data:\")\n",
    "print(X_train_encoded.toarray())\n",
    "\n",
    "print(\"Encoded test data:\")\n",
    "print(X_test_encoded.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00953bf5-43d3-4b26-bafd-af2cbb4ee4fc",
   "metadata": {},
   "source": [
    "#### One Hot Encoding or Ordinal Encoding?\n",
    "\n",
    "##### Ordinal Encoding\n",
    "- Assigns numerical labels to categorical variables with a meaningful order or hierarchy.\n",
    "\n",
    "**Example:**\n",
    "- Original Data: ['Low', 'Medium', 'High']\n",
    "- Ordinal Encoded:\n",
    "     ```\n",
    "     0\n",
    "     1\n",
    "     2\n",
    "     ```\n",
    "\n",
    "- Appropriate for categorical variables with a clear order (e.g., 'low' < 'medium' < 'high')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fbd1b-9e56-439f-a77a-6f0beb44749f",
   "metadata": {},
   "source": [
    "#### ColumnTransformer for Multiple Categories\n",
    "\n",
    "- Too many categories in a categorical variable can lead to a high-dimensional representation.\n",
    "\n",
    "- We can use `ColumnTransformer` to apply different transformations to specific columns.\n",
    "\n",
    "3. **Example:**\n",
    "   ```python\n",
    "   from sklearn.compose import ColumnTransformer\n",
    "   from sklearn.preprocessing import OneHotEncoder\n",
    "   from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "   # Different transformations to different column types\n",
    "   transformer = ColumnTransformer(\n",
    "       transformers=[\n",
    "           ('categorical', OneHotEncoder(handle_unknown='ignore'), ['category']),\n",
    "           ('text', CountVectorizer(), ['text'])\n",
    "       ])\n",
    "\n",
    "   transformed_data = transformer.fit_transform(data)\n",
    "   ```\n",
    "The **'category'** column is one-hot encoded, and the **'text'** column is vectorized using `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e4367-9490-4079-9eac-fb572146846d",
   "metadata": {},
   "source": [
    "##### Text Data Treatment vs. Categorical Variables\n",
    "\n",
    "- **Text Data:**\n",
    "  - Involves natural language processing.\n",
    "  - Requires tokenization, stemming, or lemmatization.\n",
    "\n",
    "- **Categorical Variables:**\n",
    "  - Represent discrete categories.\n",
    "  - Encoded using methods like one-hot or ordinal encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024081b9-4ff2-4602-981d-d48cf69de3b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding Bag of Words (CountVectorizer)\n",
    "\n",
    "- Bag of Words (BoW) is a fundamental technique in Natural Language Processing (NLP) used for text analysis and feature extraction.\n",
    "- BoW is a technique that converts text documents into numerical feature vectors.\n",
    "- It represents a document as a collection (or \"bag\") of words, disregarding grammar and word order but keeping track of word frequency.\n",
    "- BoW is widely used for text classification, sentiment analysis, and clustering tasks in NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f33b4c-afdf-4bb6-9a89-fb2a5a45b6c1",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1200/1*3K9GIOVLNu0cRvQap_KaRg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89eb31-467a-4770-8a78-1f99af4fbc16",
   "metadata": {},
   "source": [
    "- How **BoW** works:\n",
    "  1. Tokenization: Break down the text into individual words or tokens.\n",
    "  2. Counting: Count the occurrence of each word in the document.\n",
    "  3. Vectorization: Convert the word counts into numerical feature vectors.\n",
    "\n",
    "#### Example Text Document\n",
    "\n",
    "- Let's consider an example text document: \"The cat sat on the mat. The mat was blue.\"\n",
    "- We will apply Bag of Words to this document to convert it into a numerical representation.\n",
    "\n",
    "- The tokenization process separates the document into individual words:\n",
    "  ```\n",
    "    [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"The\", \"mat\", \"was\", \"blue\"].\n",
    "  ```\n",
    "- Counting the occurrences of each word, we get:\n",
    "\n",
    "    ```\n",
    "    {\"The\": 2,\n",
    "    \"cat\": 1,\n",
    "    \"sat\": 1,\n",
    "    \"on\": 1,\n",
    "    \"the\": 1,\n",
    "    \"mat\": 2,\n",
    "    \"was\": 1,\n",
    "    \"blue\": 1}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22574340-1dbb-4365-9cec-c67d98ff8514",
   "metadata": {},
   "source": [
    "#### CountVectorizer in Python \n",
    "\n",
    "- CountVectorizer in Python is a powerful tool that automates the process of Bag of Words.\n",
    "- It converts a collection of text documents to a matrix of token counts.\n",
    "- Let's see how to implement Bag of Words using CountVectorizer in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793c3e68-eecb-43b2-ad58-1cf1f5d0795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names (tokens): ['and' 'blue' 'cat' 'happy' 'mat' 'on' 'sat' 'the' 'was']\n",
      "Numerical representation of documents:\n",
      "[[0 0 1 0 1 1 1 2 0]\n",
      " [1 1 1 1 1 0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example text documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The mat was blue and the cat was happy.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names and the numerical representation of the documents\n",
    "print(\"Feature names (tokens):\", feature_names)\n",
    "print(\"Numerical representation of documents:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd604f29-0aea-4bd5-b552-ad45babceaa0",
   "metadata": {},
   "source": [
    "#### Applications of Bag of Words\n",
    "- Bag of Words has numerous applications in NLP, including:\n",
    "  - Text classification: Identifying the topic or sentiment of a document.\n",
    "  - Document clustering: Grouping similar documents together.\n",
    "  - Information retrieval: Ranking documents based on relevance to a query.\n",
    "\n",
    "\n",
    "#### Important hyperparameters:\n",
    "- `stop_words` : Does not take into consideration words that are not meaningful (is, the, a, ...)\n",
    "- `max_features` : Some vocabularies can be massive, this can make things SLOW. Use this to make the process faster, at least when you set up the pipeline.\n",
    "- `lowercase`: Convert all text to lowercase.\n",
    "\n",
    "#### Remember\n",
    "- Bag of Words is a powerful technique for converting text data into numerical representations.\n",
    "- `CountVectorizer` in Python simplifies the implementation of Bag of Words.\n",
    "- Understanding Bag of Words is essential for various NLP tasks, enabling efficient text analysis and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e61332-3a1e-4fd0-b07d-ceee1ee55a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
