{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d3b0ee-f491-4e51-8323-3e6635ed304a",
   "metadata": {},
   "source": [
    "### UBC Extended Learning\n",
    "#### Instructor: Socorro Dominguez\n",
    "#### Module 07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbb478-a185-4d92-a7d2-b4ed3116ce6a",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "- [] Explain why accuracy is not always the best metric in ML.\n",
    "- [] Explain components of a confusion matrix.\n",
    "- [] Define precision, recall, and f1-score \n",
    "- [] Identify whether there is class imbalance and whether you need to deal with it.\n",
    "- [] Explain class_weight and use it to deal with data imbalance.\n",
    "- [] Appropriately select a scoring metric given a regression problem.\n",
    "- [] Interpret and communicate the meanings of different scoring metrics on regression problems. MSE, RMSE,  R2 , MAPE.\n",
    "- [] Apply different scoring functions with cross_validate and GridSearchCV and RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23285e-b50b-484a-9076-af27090ada4f",
   "metadata": {},
   "source": [
    "## Accuracy: Not Always the Best Metric\n",
    "- Accuracy is a commonly used metric to evaluate model performance, but it may not always be the best choice.\n",
    "\n",
    "- **Example:** Let's say we have a model that predicts whether an email is spam or not. Out of 1000 emails, 950 are not spam (negative) and 50 are spam (positive).\n",
    "  - The model correctly classifies 920 non-spam emails and 10 spam emails. \n",
    "  - Accuracy = (920 + 10) / 1000 = 93%\n",
    "- At first glance, 93% accuracy seems good. However, the model misses 40 spam emails (false negatives), which could be harmful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295766e0-e499-44b6-b0e2-f054168a824c",
   "metadata": {},
   "source": [
    "## Components of a Confusion Matrix\n",
    "- The confusion matrix is a table used to evaluate the performance of a classification model.\n",
    "\n",
    "- **Example:** Continuing from the previous example:\n",
    "  \n",
    "  |               | Predicted Negative | Predicted Positive |\n",
    "  |---------------|--------------------|--------------------|\n",
    "  | Actual Negative (Non-Spam) | 920 (True Negative) | 30 (False Positive) |\n",
    "  | Actual Positive (Spam)     | 40 (False Negative) | 10 (True Positive)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463c0b8-6fc7-460c-98be-42967df3af22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Precision, Recall, and F1-Score\n",
    "- These metrics provide deeper insights into a model's performance, especially in imbalanced datasets.\n",
    "- **Example:** Let's use the confusion matrix from the previous example to calculate precision, recall, and F1-score:\n",
    "  \n",
    "  - **Precision** = TP / (TP + FP) = 10 / (10 + 30) ≈ 25%\n",
    "    - Out of the emails predicted as spam, only 25% are actually spam.\n",
    "  - **Recall** = TP / (TP + FN) = 10 / (10 + 40) ≈ 20%\n",
    "    - The model correctly identifies only 20% of the actual spam emails.\n",
    "  - **F1-Score** = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    - F1-Score ≈ 22.22%\n",
    "    - The F1-Score balances precision and recall, providing a single metric to assess model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52433d50-50f4-4e88-8acf-1ff7d256e0d3",
   "metadata": {},
   "source": [
    "### Recall or Precision?\n",
    "\n",
    "For example, let's think about the medical field, there are cases when we should prioritze recall.\n",
    "\n",
    "- In critical medical conditions, where FN can have severe consequences or even be life-threatening.\n",
    "\n",
    "- For diseases that are highly contagious (think of COVID-19), where early detection is crucial for containment and prevention of further spread.\n",
    "\n",
    "There are cases where Precision could be more important:\n",
    "\n",
    "- In situations where false positives have significant negative impacts, such as unnecessary treatments, interventions, or psychological distress for healthy individuals.\n",
    "\n",
    "In most cases, you may want to prioritize both. You don't want to miss a cancer patient, but you don't want to give radioterapy to a healthy individual. The problem that you have, will tell you which measure you should prioritize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7c177-1574-47bc-802f-01a72ab87ff1",
   "metadata": {},
   "source": [
    "## Dealing with Class Imbalance\n",
    "- Class imbalance occurs when one class dominates the dataset, leading to biased model performance.\n",
    "\n",
    "- **Example**: Let's consider a medical diagnosis model for a rare disease. Out of 1000 patients, only 10 have the disease (positive class).\n",
    "- Dealing with imbalance: Resampling, using different evaluation metrics, and using `class_weight`.\n",
    "\n",
    "### Class Weight to Handle Imbalance\n",
    "- `class_weight` is a technique used to assign higher weights to minority classes during model training.\n",
    "- **Example**: In our medical diagnosis model, we can set `class_weight='balanced'` to give more importance to the 10 positive cases during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0e478-09b4-4985-af0f-0d77e98502ad",
   "metadata": {},
   "source": [
    "\n",
    "## Scoring Metrics for Regression Problems\n",
    "- Regression problems require different evaluation metrics than classification.\n",
    "- **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values.\n",
    "  - **Example**: Let's predict house prices, and we have the following actual and predicted prices:\n",
    "    - Actual Prices: [200, 300, 400, 500]\n",
    "    - Predicted Prices: [180, 280, 350, 480]\n",
    "    - MSE = ((200-180)^2 + (300-280)^2 + (400-350)^2 + (500-480)^2) / 4 = 1050\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**: The square root of MSE, providing errors in the original units.\n",
    "  - **Example**: RMSE ≈ √1050 ≈ 32.40 (measured in the same unit as house prices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c3cd7-48e8-45fd-a9e1-840a56f70305",
   "metadata": {},
   "source": [
    "## Interpretation of Regression Scoring Metrics\n",
    "- **R-squared (R2)**: Indicates the proportion of variance in the target variable explained by the model.\n",
    "  - **Example**: R2 = 1 - (MSE of the model / MSE of the mean model)\n",
    "  - R2 = 1 - (1050 / ((200+300+400+500)/4)^2) ≈ 0.82\n",
    "  - An R2 value of 0.82 suggests that the model explains 82% of the variance in house prices.\n",
    "  - **Note**: In scikit-learn, R2 can sometimes be negative when the model's performance is worse than a simple horizontal line (mean model). This indicates that the model is a poor fit for the data.\n",
    "\n",
    "- **Mean Absolute Percentage Error (MAPE)**: Measures the percentage difference between predicted and actual values\n",
    "  - **Example**: Let's consider the same house price predictions as before:\n",
    "    - MAPE = ((|200-180|/200) + (|300-280|/300) + (|400-350|/400) + (|500-480|/500)) / 4 ≈ 0.0725 or 7.25% \n",
    "    This means, on average, the model's predictions have an error of about 7.25% relative to the actual values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a7233-ed8c-469c-a854-224cf100d34c",
   "metadata": {},
   "source": [
    "Remember, when choosing evaluation metrics, consider the specific characteristics of your dataset and the goals of your machine learning project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80a50d-167f-4852-89d1-24052fc70089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
